{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7v0MF6Krmd0l"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=100):\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        # Initialize weights and bias to small random values or zero\n",
        "        self.w1 = random.uniform(-0.5, 0.5)\n",
        "        self.w2 = random.uniform(-0.5, 0.5)\n",
        "        self.b = random.uniform(-0.5, 0.5)\n",
        "\n",
        "    def predict(self, x1, x2):\n",
        "        # Weighted sum (Evidence Aggregation): z = w1*x1 + w2*x2 + b\n",
        "        z = (self.w1 * x1) + (self.w2 * x2) + self.b\n",
        "        # Decision Rule (Activation Function)\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def train(self, dataset):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for x1, x2, y_true in dataset:\n",
        "                # Predict output\n",
        "                y_hat = self.predict(x1, x2)\n",
        "\n",
        "                # Compute error\n",
        "                error = y_true - y_hat\n",
        "\n",
        "                if error != 0:\n",
        "                    # Update weights\n",
        "                    self.w1 += self.lr * error * x1\n",
        "                    self.w2 += self.lr * error * x2\n",
        "                    # Update bias\n",
        "                    self.b += self.lr * error\n",
        "                    total_error += abs(error)\n",
        "\n",
        "            # Convergence: stop if no further updates occur [cite: 175, 176]\n",
        "            if total_error == 0:\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gate_task(gate_name, dataset):\n",
        "    print(f\"--- Training {gate_name} Gate ---\")\n",
        "\n",
        "    p = Perceptron(learning_rate=0.1, epochs=200)\n",
        "    p.train(dataset)\n",
        "\n",
        "    print(f\"Final Weights: w1={p.w1:.4f}, w2={p.w2:.4f}\")\n",
        "    print(f\"Final Bias: b={p.b:.4f}\")\n",
        "    print(\"Predictions:\")\n",
        "    for x1, x2, y in dataset:\n",
        "        pred = p.predict(x1, x2)\n",
        "        print(f\"  Input: ({x1}, {x2}) -> Expected: {y}, Predicted: {pred}\")\n"
      ],
      "metadata": {
        "id": "8rovekd7pZ2P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for Logic Gates\n",
        "datasets = {\n",
        "    \"AND\":  [(0,0,0), (0,1,0), (1,0,0), (1,1,1)],\n",
        "    \"OR\":   [(0,0,0), (0,1,1), (1,0,1), (1,1,1)],\n",
        "    \"NAND\": [(0,0,1), (0,1,1), (1,0,1), (1,1,0)],\n",
        "    \"NOR\":  [(0,0,1), (0,1,0), (1,0,0), (1,1,0)],\n",
        "    \"XOR\":  [(0,0,0), (0,1,1), (1,0,1), (1,1,0)]\n",
        "}"
      ],
      "metadata": {
        "id": "bqeG4Q08pcYw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for gate, data in datasets.items():\n",
        "    run_gate_task(gate, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24V3jJnLphyF",
        "outputId": "4af2a44f-609a-4269-f768-0a6064e134a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training AND Gate ---\n",
            "Final Weights: w1=0.1706, w2=0.1033\n",
            "Final Bias: b=-0.2275\n",
            "Predictions:\n",
            "  Input: (0, 0) -> Expected: 0, Predicted: 0\n",
            "  Input: (0, 1) -> Expected: 0, Predicted: 0\n",
            "  Input: (1, 0) -> Expected: 0, Predicted: 0\n",
            "  Input: (1, 1) -> Expected: 1, Predicted: 1\n",
            "--- Training OR Gate ---\n",
            "Final Weights: w1=0.1374, w2=0.2943\n",
            "Final Bias: b=-0.0197\n",
            "Predictions:\n",
            "  Input: (0, 0) -> Expected: 0, Predicted: 0\n",
            "  Input: (0, 1) -> Expected: 1, Predicted: 1\n",
            "  Input: (1, 0) -> Expected: 1, Predicted: 1\n",
            "  Input: (1, 1) -> Expected: 1, Predicted: 1\n",
            "--- Training NAND Gate ---\n",
            "Final Weights: w1=-0.1902, w2=-0.0437\n",
            "Final Bias: b=0.2224\n",
            "Predictions:\n",
            "  Input: (0, 0) -> Expected: 1, Predicted: 1\n",
            "  Input: (0, 1) -> Expected: 1, Predicted: 1\n",
            "  Input: (1, 0) -> Expected: 1, Predicted: 1\n",
            "  Input: (1, 1) -> Expected: 0, Predicted: 0\n",
            "--- Training NOR Gate ---\n",
            "Final Weights: w1=-0.0625, w2=-0.3628\n",
            "Final Bias: b=0.0606\n",
            "Predictions:\n",
            "  Input: (0, 0) -> Expected: 1, Predicted: 1\n",
            "  Input: (0, 1) -> Expected: 0, Predicted: 0\n",
            "  Input: (1, 0) -> Expected: 0, Predicted: 0\n",
            "  Input: (1, 1) -> Expected: 0, Predicted: 0\n",
            "--- Training XOR Gate ---\n",
            "Final Weights: w1=-0.1538, w2=-0.0361\n",
            "Final Bias: b=0.0821\n",
            "Predictions:\n",
            "  Input: (0, 0) -> Expected: 0, Predicted: 1\n",
            "  Input: (0, 1) -> Expected: 1, Predicted: 1\n",
            "  Input: (1, 0) -> Expected: 1, Predicted: 0\n",
            "  Input: (1, 1) -> Expected: 0, Predicted: 0\n"
          ]
        }
      ]
    }
  ]
}